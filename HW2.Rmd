---
title: "HW2 STA521 Fall18"
author: '[Yiwei Gong yg140 ywgej9]'
date: "Due September 23, 2018 5pm"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(GGally)
library(alr3)
library(car)
library(dplyr)
library(knitr)
library(ggplot2)
```

This exercise involves the UN data set from `alr3` package. Install `alr3` and the `car` packages and load the data to answer the following questions adding your code in the code chunks.  Please add appropriate code to the chunks to suppress messages and warnings as needed once you are sure the code is working properly and remove instructions if no longer needed. Figures should have informative captions. Please switch the output to pdf for your final version to upload to Sakai. **Remove these instructions for final submission**


## Exploratory Data Analysis


```{r data, echo=FALSE}
library(alr3)
data(UN3, package="alr3")
help(UN3)
library(car)
```


1. Create a summary of the data.  How many variables have missing data?  Which are quantitative and which are qualtitative?

```{r summary, echo=FALSE}
library(tidyr)
summary(UN3)
data(UN3)
UN.nna = UN3 %>% drop_na()  ## data without NA's. Since model fitting process omit NA automatically, fitting UN3 and UN.nna will give the same result.
```
__Answer__: There are 7 variables having missing data. ModernC, Change, PPgdp, Frate, Pop, Fertility and Purban all are quantitative.  
  

2. What is the mean and standard deviation of each quantitative predictor?  Provide in a nicely formatted table.

```{r kable, echo=FALSE}
Mean = c(mean(UN3$ModernC, na.rm=T),
         mean(UN3$Change, na.rm=T),
         mean(UN3$PPgdp, na.rm=T),
         mean(UN3$Frate, na.rm=T),
         mean(UN3$Pop, na.rm=T),
         mean(UN3$Fertility, na.rm=T),
         mean(UN3$Purban, na.rm=T))
Standard_err = c(sd(UN3$ModernC, na.rm=T),
                 sd(UN3$Change,na.rm=T),
                 sd(UN3$PPgdp, na.rm=T),
                 sd(UN3$Frate, na.rm=T),
                 sd(UN3$Pop, na.rm=T),
                 sd(UN3$Fertility, na.rm=T),
                 sd(UN3$Purban, na.rm=T))
stat.UN = cbind(Mean, Standard_err)
kable(t(stat.UN), col.names=c('ModernC', 'Change', 'PPgdp', 'Frate', 'Pop', 'Fertility', 'Purban'), format='markdown')
```


3. Investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings regarding trying to predict `ModernC` from the other variables.  Are there potential outliers, nonlinear relationships or transformations that appear to be needed based on your graphical EDA?

```{r scatter plot, echo=F}
ggpairs(UN.nna, columns=c(1, 2, 3, 4, 5, 6, 7), progress=F, title='Scatter Plots and Correlations among Variables')
```

__Answer__: There seem downward linear trendings in ModernC when Change and Fertility increase, but ModernC seems to increase together with Purban, and when PPgdp increases, ModernC seems to increase exponentially. Two points are suspicious to be outliers. Transformation of Pop may be required as the range of Pop is considerably large.  
  

## Model Fitting

4.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response and all other variables as the predictors, using the formula `ModernC ~ .`, where the `.` includes all remaining variables in the dataframe.  Create  diagnostic residual plot from the linear model object and comment on results regarding assumptions.  How many observations are used in your model fitting?

```{r initial linear, echo=FALSE}
lm1 = lm(ModernC ~ Change + PPgdp + Frate + Pop + Fertility + Purban, data=UN.nna)
summary(lm1)
nrow(UN.nna)  ## na.omit(UN3) is the same as UN.nna.
par(mfrow=c(2,2))
plot(lm1)
```
The Residuals vs Fitted plot suggests constant and 0 expectation of residuals, though fluctuated in the middle, and the Scale-Location plot shows possible violation of constant variance. The Normal QQ plot fits well in the middle, though the point of Poland seems strange. The Residuals vs Leverage suggests there is no highly influential point. 210 observations exist, but only 125 observations are used in this model fitting, since they don't have missing values.  


5. Examine added variable plots `car::avPlot` or `car::avPlots`  for your model above. Are there any plots that suggest that transformations are needed for any of the terms in the model? Describe. Is it likely that any of the localities are influential for any of the terms?  Which localities?  Which terms?  

```{r avplots, echo=F}
car::avPlots(lm(ModernC ~ Change + PPgdp + Frate + Pop + Fertility + Purban, data=UN.nna))
```

__Answer__: Transformation seems needed for Pop, as most of the points cluster together except two. PPgdp might need transformation as well. China and India in terms of Pop seem influential, though the Residual vs Leverage does not suggest that.

6.  Using the Box-Tidwell  `car::boxTidwell` or graphical methods find appropriate transformations of the predictor variables to be used as predictors in the linear model.  If any predictors are negative, you may need to transform so that they are non-negative.  Describe your method and  the resulting transformations.

```{r boxtidwell, echo=F}
car::boxTidwell(ModernC~Pop + PPgdp, ~Change + Frate + Fertility + Purban, data=UN.nna)
gPop.non = ggplot(data=UN.nna, aes(x=Pop, y=ModernC)) + geom_point()
gPop.trans = ggplot(data=UN.nna, aes(x=log(Pop), y=ModernC)) + geom_point()
gPPgdp.non = ggplot(data=UN.nna, aes(x=PPgdp, y=ModernC)) + geom_point()
gPPgdp.trans = ggplot(data=UN.nna, aes(x=log(PPgdp), y=ModernC)) + geom_point()
list(gPop.non, gPop.trans)
list(gPPgdp.non, gPPgdp.trans)
```
__Answer__: When boxTidwell is applied to PPgdp and Pop, MLE are closer to 0, so may be a log transformation, but there does not seem significant evidence for transformation, since both p-values are pretty large. However, the scatterplots shows that (log(Pop), ModernC) and (log(PPgdp), ModernC) illustrate clearer linear relationship. Therefore, log(Pop) and log(PPgdp) seem proper candidates for transformation.



7. Given the selected transformations of the predictors, select a transformation of the response using `MASS::boxcox` or `car::boxCox` and justify.


```{r boxcox, echo=FALSE}
MASS::boxcox(lm1)
```

__Answer__: The Boxcox plot suggests there might be some transformation changing ModernC's power to some number close to 0.8. However, for simplicity and interpretation, there may not be any transformation of Y required since 1 is also in the range for available $\lambda$.


8.  Fit the regression using the transformed variables.  Provide residual plots and added variables plots and comment.  If you feel that you need additional transformations of either the response or predictors, repeat any steps until you feel satisfied.
```{r newfit, echo=F}
lm2 = lm(ModernC ~ log(Pop) + log(PPgdp) + Change + Frate + Fertility + Purban, data=UN.nna)
summary(lm2)
par(mfrow=c(2, 2))
plot(lm2)
car::avPlots(lm2)
```

__Answer__: After refitting the model with log(Pop) and log(PPgdp), there is improvement in the Scale-Location plot. There seems no potential highly influential point, after the transformation.  


9. Start by finding the best transformation of the response and then find transformations of the predictors.  Do you end up with a different model than in 8?


```{r cox1, echo=FALSE}
MASS::boxcox(lm1)
```

__Answer__: The Boxcox suggests no transformation for ModernC since again, 1 is in the interval. Therefore, the following steps for boxTidwell will be the same as before (Question 6 to Q8). Thus there is no difference between these two procedures.  


10.  Are there any outliers or influential points in the data?  Explain.  If so, refit the model after removing any outliers and comment on residual plots.

```{r outliers, echo=FALSE}
library(tibble)
pval = 2*(1 - pt(abs(rstudent(lm2)), lm2$df -1))
rownames(UN.nna)[pval < 0.05/nrow(UN.nna)]
```

__Answer__: The Residuals vs Fitted, Normal Q-Q, and Scale-Location plots suggest there are three potential outliers, which are Poland, Azerbajian, and Cook Island. However, Bonferroni test suggests that there is no outlier.  


## Summary of Results

11. For your final model, provide summaries of coefficients with 95% confidence intervals in a nice table with interpretations of each coefficient.  These should be in terms of the original units! 


```{r final, echo=FALSE}
library(xtable)
lm3 = lm(ModernC ~ log(Pop) + log(PPgdp) + Change + Fertility + Frate, data=UN.nna)
summary(lm3)
car::avPlots(lm3)
interpretation = c("The base value of ModernC without any predictor", "10% increase will increase ModernC by 0.137%", "10% increase will increase ModernC by 0.461%", "1% increase will increase ModernC by 4.698%", "1% increase will decrease ModernC by 9.278%", "1 unit increase will increase ModernC by 0.200%")
kable(cbind(confint(lm3), interpretation))
```

__Answer__: The summary suggests that the transformed model satisfies $$ModernC = 4.102 + 1.441log(Pop) + 4.859log(PPgdp) + 4.698Change - 9.278Fertility + 0.200Frate$$ This means, 10% increase in Pop will lead to ModernC's increase by 1.441*log1.1 percent, which is 0.137%, and 10% increase in PPgdp will lead to 0.461 (4.859*log(1.1)) percent in ModernC. 1 unit increase in Change and Fertility will increase ModernC by 4.698% and 0.200% respectively, while 1 percent increase in Frate will decrease ModernC by 9.278 percents.  

12. Provide a paragraph summarizing your final model  and findings suitable for the US envoy to the UN after adjusting for outliers or influential points.   You should provide a justification for any case deletions in your final model


```{r final.inf, echo=FALSE}
par(mfrow=c(2, 2))
plot(lm3)
```

According to Cook's distance in Residuals vs Leverage plot, there is no point with this distance over 1. Therefore, I don't think there is any influential point so no deletion, same model as in Q11.


## Methodology

    
13. Prove that the intercept in the added variable scatter plot will always be zero.  _Hint:  use the fact that if $H$ is the project matrix which contains a column of ones, then $1_n^T (I - H) = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept._

__Answer__:
\[
\begin{aligned}
e_Y &= \hat{\beta}_0 + \hat{\beta}_1e_{x_i}\\
\mathbf{1}_n^T e &= \mathbf{1}_n^T (I-H)Y \qquad\text{times row vector 1 on both sides}\\
&=[\mathbf{1}_n^T (I-H)] Y\\
&=0*Y  \qquad\text{using hint}\\
&=0\qquad\text{(1)}\\
e_Y = (I-H)Y &= \hat{\beta}_0 + \hat{\beta}_1\underbrace{e_{x_i}}_{(I-H)X_i} \qquad\text{(2)}\\
\text{(1)} \implies 0 &= \mathbf{1}_n^T (I-H)Y\\
&= \mathbf {1}_n^T[\hat{\beta}_0 + \hat{\beta}_1 e_{x_i}]\qquad\text{by (2)}\\
&= \mathbf{1}_n^T \hat{\beta}_0 + \mathbf{1}_n^T \hat{\beta}_1 (I-H) x_{i}\\
&= \mathbf{1}_n^T \hat{\beta}_0 + \hat{\beta}_1[\mathbf{1}_n^T(I-H)] x_{i}\\
\implies 0 = \mathbf{1}_n^T \hat{\beta}_0 + 0 \qquad\text{by hint}\\
\implies\mathbf{1}_n^T \hat{\beta}_0 = 0 \implies \hat\beta_0 = 0\\
\end{aligned}
\]
Therefore, the intercept of avplots are always zero.

14. For multiple regression with more than 2 predictors, say a full model given by `Y ~ X1 + X2 + ... Xp`   we create the added variable plot for variable `j` by regressing `Y` on all of the `X`'s except `Xj` to form `e_Y` and then regressing `Xj` on all of the other X's to form `e_X`.  Confirm that the slope in a manually constructed added variable plot for one of the predictors  in Ex. 10 is the same as the estimate from your model. 
```{r 14, echo=F}
e_Y = residuals(lm(ModernC~log(Pop)+log(PPgdp)+Fertility+Change, UN.nna))
e_X = residuals(lm(Frate ~ log(Pop)+log(PPgdp)+Fertility+Change, UN.nna))
df = data.frame(e_Y=e_Y, e_X=e_X)
ggplot(data=df, aes(x=e_X, y=e_Y)) +
  geom_point()+
  geom_smooth(method='lm', se=F)
lm3$coefficients["Frate"]
Original = summary(lm3)$coefficients['Frate',c('Estimate','t value')]
Partial = summary(lm(e_Y~e_X))$coefficients['e_X', c('Estimate', 't value')]
kable(rbind(Original, Partial))
```

Two regressions give the same coefficients, though tiny different t-values, which may come from the change in degree of freedom.